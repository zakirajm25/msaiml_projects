{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c831f2",
   "metadata": {},
   "source": [
    "# Low-Rank Adaptation (LoRA) Fine-Tuning Walkthrough\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for adapting a transformer-based text classifier with LoRA (Low-Rank Adaptation). We'll cover dataset preparation, applying LoRA to the model, training, evaluation, and ideas for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3f831",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- **Setup:** install required libraries and initialize the environment.\n",
    "- **Step 1 ? Dataset preparation:** load a sentiment dataset (with a synthetic fallback) and prepare it for training.\n",
    "- **Step 2 ? Apply LoRA:** wrap a base transformer classifier with a LoRA adapter.\n",
    "- **Step 3 ? Fine-tune:** train the LoRA-augmented model efficiently.\n",
    "- **Step 4 ? Evaluate:** measure accuracy, F1, and inspect predictions.\n",
    "- **Step 5 ? Optimize:** explore levers for improving LoRA performance and portability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042134f",
   "metadata": {},
   "source": [
    "### Environment & Dependency Setup\n",
    "Run the next cell if you need to install packages. Restart the kernel after installing to ensure newly installed libraries are picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea1e54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:16.275115Z",
     "iopub.status.busy": "2025-10-15T07:26:16.274159Z",
     "iopub.status.idle": "2025-10-15T07:26:22.991067Z",
     "shell.execute_reply": "2025-10-15T07:26:22.989121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade accelerate datasets evaluate peft transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674058e",
   "metadata": {},
   "source": [
    "### Imports and configuration\n",
    "We collect every dependency in one place and set deterministic behavior for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19ca29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:22.995542Z",
     "iopub.status.busy": "2025-10-15T07:26:22.994534Z",
     "iopub.status.idle": "2025-10-15T07:26:32.824634Z",
     "shell.execute_reply": "2025-10-15T07:26:32.823618Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC VISION\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "os.environ['USE_TF'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "label_names = [\"negative\", \"positive\"]  # default; will update once data is loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f2d5c",
   "metadata": {},
   "source": [
    "## Step 1 ? Prepare your dataset\n",
    "We'll start with IMDb sentiment data. If it's unavailable (e.g., no network access), we automatically fall back to a small synthetic dataset so the end-to-end flow still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0c3b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:32.828892Z",
     "iopub.status.busy": "2025-10-15T07:26:32.827857Z",
     "iopub.status.idle": "2025-10-15T07:26:43.188726Z",
     "shell.execute_reply": "2025-10-15T07:26:43.187726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:IMDb dataset loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_synthetic_dataset(num_samples: int = 200):\n",
    "    \"\"\"Create a tiny sentiment dataset when external downloads are blocked.\"\"\"\n",
    "    positives = [\n",
    "        \"I absolutely loved this movie, it was fantastic!\",\n",
    "        \"Great performances and an uplifting story.\",\n",
    "        \"The product quality exceeded my expectations.\",\n",
    "        \"Customer support was helpful and quick to respond.\",\n",
    "    ]\n",
    "    negatives = [\n",
    "        \"This was a waste of time, I hated it.\",\n",
    "        \"Terrible experience, would not recommend to anyone.\",\n",
    "        \"Quality was disappointing and the item broke quickly.\",\n",
    "        \"Customer service never replied to my emails.\",\n",
    "    ]\n",
    "    texts, labels = [], []\n",
    "    for _ in range(num_samples // 2):\n",
    "        texts.append(random.choice(positives))\n",
    "        labels.append(1)\n",
    "        texts.append(random.choice(negatives))\n",
    "        labels.append(0)\n",
    "    data = {\"text\": texts, \"label\": labels}\n",
    "    full_dataset = Dataset.from_dict(data)\n",
    "    return full_dataset.shuffle(seed=SEED)\n",
    "\n",
    "\n",
    "try:\n",
    "    raw_datasets = load_dataset(\"imdb\")\n",
    "    logger.info(\"IMDb dataset loaded successfully.\")\n",
    "    # Downsample for faster experiments.\n",
    "    raw_datasets[\"train\"] = raw_datasets[\"train\"].shuffle(seed=SEED).select(range(600))\n",
    "    raw_datasets[\"test\"] = raw_datasets[\"test\"].shuffle(seed=SEED).select(range(240))\n",
    "    label_names = raw_datasets[\"train\"].features[\"label\"].names or label_names\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    logger.warning(\"Falling back to synthetic dataset because of: %s\", exc)\n",
    "    synthetic = build_synthetic_dataset(num_samples=160)\n",
    "    split = synthetic.train_test_split(test_size=0.2, seed=SEED)\n",
    "    raw_datasets = DatasetDict({\"train\": split[\"train\"], \"test\": split[\"test\"]})\n",
    "\n",
    "num_labels = len(set(raw_datasets[\"train\"][\"label\"]))\n",
    "label_names = label_names[:num_labels]\n",
    "print(raw_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656047eb",
   "metadata": {},
   "source": [
    "### Inspect a sample\n",
    "It's good practice to sanity check a few examples before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f6649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:43.192825Z",
     "iopub.status.busy": "2025-10-15T07:26:43.191824Z",
     "iopub.status.idle": "2025-10-15T07:26:43.204175Z",
     "shell.execute_reply": "2025-10-15T07:26:43.203163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...',\n",
       "  'This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.',\n",
       "  'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.'],\n",
       " 'label': [1, 1, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f996ad",
   "metadata": {},
   "source": [
    "## Step 1b ? Tokenize and prepare features\n",
    "We tokenize the text, keep attention masks, and retain label ids. Padding is deferred to the data collator for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef66ec90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:43.208215Z",
     "iopub.status.busy": "2025-10-15T07:26:43.207177Z",
     "iopub.status.idle": "2025-10-15T07:26:44.989310Z",
     "shell.execute_reply": "2025-10-15T07:26:44.988302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 3463.09 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 3038.55 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 2759.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ec1b4",
   "metadata": {},
   "source": [
    "### Metrics helper\n",
    "We combine accuracy and weighted F1. Evaluate will download metric definitions once and cache them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ef702e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:44.992301Z",
     "iopub.status.busy": "2025-10-15T07:26:44.992301Z",
     "iopub.status.idle": "2025-10-15T07:26:52.073684Z",
     "shell.execute_reply": "2025-10-15T07:26:52.072563Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1_weighted\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d37f9",
   "metadata": {},
   "source": [
    "## Step 2 ? Apply LoRA to the model\n",
    "We load a compact base model and attach LoRA adapters on the attention projections. Only the low-rank adapter parameters will be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3386c9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:52.077853Z",
     "iopub.status.busy": "2025-10-15T07:26:52.077853Z",
     "iopub.status.idle": "2025-10-15T07:26:52.722775Z",
     "shell.execute_reply": "2025-10-15T07:26:52.721518Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.15,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02948f00",
   "metadata": {},
   "source": [
    "## Step 3 ? Fine-tune the model with LoRA\n",
    "Training arguments are purposely lightweight so you can iterate quickly on a CPU or single GPU machine. Adjust batch size and epochs as your hardware allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4c152b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:52.727773Z",
     "iopub.status.busy": "2025-10-15T07:26:52.726772Z",
     "iopub.status.idle": "2025-10-15T07:26:59.306103Z",
     "shell.execute_reply": "2025-10-15T07:26:59.304987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC VISION\\AppData\\Local\\Temp\\ipykernel_19748\\1024455725.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.core.run:Could not load the run context. Logging offline\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"artifacts/distilbert-imdb-lora\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66841b8",
   "metadata": {},
   "source": [
    "### Train\n",
    "Execute the cell below to start LoRA fine-tuning. Training artifacts land under `artifacts/` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010cf070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:26:59.309320Z",
     "iopub.status.busy": "2025-10-15T07:26:59.309320Z",
     "iopub.status.idle": "2025-10-15T07:34:10.078892Z",
     "shell.execute_reply": "2025-10-15T07:34:10.078892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC VISION\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 07:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>0.587220</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.6564\n",
      "Attempted to log scalar metric grad_norm:\n",
      "2.9516141414642334\n",
      "Attempted to log scalar metric learning_rate:\n",
      "7.761194029850747e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.5872203707695007\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.7125\n",
      "Attempted to log scalar metric eval_f1_weighted:\n",
      "0.7027028834323008\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "48.024\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "4.998\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.312\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric train_runtime:\n",
      "430.3851\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "1.394\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "0.174\n",
      "Attempted to log scalar metric total_flos:\n",
      "40557717504000.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.6436113866170248\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =    37772GF\n",
      "  train_loss               =     0.6436\n",
      "  train_runtime            = 0:07:10.38\n",
      "  train_samples            =        600\n",
      "  train_samples_per_second =      1.394\n",
      "  train_steps_per_second   =      0.174\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(tokenized_datasets[\"train\"])\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325e1a4",
   "metadata": {},
   "source": [
    "## Step 4 ? Evaluate the LoRA-fine-tuned model\n",
    "We evaluate on the hold-out split, compute classification metrics, and inspect a few predictions to ensure qualitative quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99d9225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:34:10.083825Z",
     "iopub.status.busy": "2025-10-15T07:34:10.083825Z",
     "iopub.status.idle": "2025-10-15T07:34:57.511690Z",
     "shell.execute_reply": "2025-10-15T07:34:57.509663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC VISION\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.5872203707695007\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.7125\n",
      "Attempted to log scalar metric eval_f1_weighted:\n",
      "0.7027028834323008\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "47.3901\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "5.064\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.317\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.7125\n",
      "  eval_f1_weighted        =     0.7027\n",
      "  eval_loss               =     0.5872\n",
      "  eval_runtime            = 0:00:47.39\n",
      "  eval_samples_per_second =      5.064\n",
      "  eval_steps_per_second   =      0.317\n",
      "{'eval_loss': 0.5872203707695007, 'eval_accuracy': 0.7125, 'eval_f1_weighted': 0.7027028834323008, 'eval_runtime': 47.3901, 'eval_samples_per_second': 5.064, 'eval_steps_per_second': 0.317, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(eval_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0e629",
   "metadata": {},
   "source": [
    "### Classification report and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29fd269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:34:57.517216Z",
     "iopub.status.busy": "2025-10-15T07:34:57.517216Z",
     "iopub.status.idle": "2025-10-15T07:35:46.040194Z",
     "shell.execute_reply": "2025-10-15T07:35:46.039297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC VISION\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.53      0.66       125\n",
      "         pos       0.64      0.91      0.75       115\n",
      "\n",
      "    accuracy                           0.71       240\n",
      "   macro avg       0.75      0.72      0.70       240\n",
      "weighted avg       0.76      0.71      0.70       240\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\n",
      "Prediction: pos (confidence=64.03%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I saw Le Conseguenze Dell'Amore on the 2005 Rotterdam Filmfestival, It was the first of ten films I saw there.<br /><br />Le Conseguenze has left the most powerful impression of the ten films. From the first shot, you know the movie is going to be something special. The beautiful cinematography left me in awe of what can be done with a camera. The music is also on par with the visuals, complementing the colorful and stylish architecture-like images.<br /><br />Toni Servillo plays the main character in the film, Titta. He's a tax expert gone wrong who lives in a hotel. Every week, he brings a suitcase with money to a bank and the story plays around this.<br /><br />He is always very controlled and shows almost no emotion to anyone; Looks calculated and well-dressed. He has a habit of ignoring people who are of no significance to him. For example Sofia (played very nicely by Olivia Magnani), who works as a barmaid in the hotel where he lives. Although she's been working in the hotel for two years, he never greets her, even if she does greet him. On one day she confronts him with this and the next day he sits at the bar, instead of his usual spot at a window. From here the story really begins, and will unfold in a strong tale of love, sacrifice and the mafia.<br /><br />I won't spoil the rest of the film. See this film if you love stylish movies like ones from David Lynch, The Godfather, etc. Don't see this if you're an action-buff.\n",
      "Prediction: pos (confidence=73.08%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I see that C. Thomas Howell has appeared in many movies since his heyday in the 80s as an accomplished young actor.<br /><br />I bought this DVD because it was cheap and in part for the internet-related plot and to see how much older C. Thomas Howell is; I do not recall seeing him in any movies since the 1980s.<br /><br />In just a few words: what a very big disappointment. I give some low budget movies a chance, but this one started out lame. Within the first 15 minutes of the movie, this elusive woman is chatting with an Asian guy in a chatroom. They basically stimulate themselves to their own chat, she then insists on meeting the participant in person. She meets him, has sex, ties him up and then murders him in cold blood. The plot then deteriorates further.<br /><br />The plot is thin and flimsy and the acting is very stiff. Do not bother renting it much less purchasing it, even if it is in the $1 DVD bin. I plan to take my copy of the DVD to Goodwill. I am truly amazed that any of the prior reviewers here gave this movie a bad rating.\n",
      "Prediction: neg (confidence=50.12%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: This is a cute little French silent comedy about a man who bets another that he can't stay in this castle for one hour due to its being haunted. And, once the guy enters the house, it looks much more like a crazed fun house or maybe like the after-effects of LSD!! While there ARE ghosts and skeletons, there is a weird menagerie of animals, odd special effects and gags as well. It's awfully hard to describe but the visuals alone make the film worth seeing. HOWEVER, understand that the self-indulgent director also had many \"funny gags\" that totally fell flat and hurt the movie. His \"camera tricks\" weren't so much tricky but annoying and stupid. IGNORE THESE AND KEEP WATCHING--it does get better. The film is fast paced, funny and worth seeing. In particular, I really liked watching the acting and mugging of Max Linder--he was so expressive and funny! Too bad he is virtually forgotten today. For an interesting but very sad read, check out the IMDb biography on him.\n",
      "Prediction: pos (confidence=57.36%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Like \"The Blair Witch Project\" before it, \"Hatchet\" has garnered its own fair share of publicity from the bottom-on-up (as an avid reader of Fangoria Magazine, the full-page ads are hard to miss); even after its middling theatrical run, the film is bound to subsist solely on the hype surrounding it, and will probably turn into a cult item at some point. With a MySpace URL and a mighty (if puzzlingly subjective) promise of preserving so-called \"old school American horror,\" \"Hatchet\" will draw a lot of curiosity seekers with its DVD release (where that claim is emblazoned on the disc itself). Perhaps it was the large-print blurb from Ain't It Cool News on the ads that caused me to approach the film with some trepidation (it seems that Harry Knowles and his minions will approve of any film for VIP passes and free food), but \"Hatchet\" makes me question what writer-director Adam Green's idea of \"old school American horror\" really is: based on the evidence here, it means the insipid, late-'80s rip-offs of \"Friday the 13th\" and \"Deliverance.\" The characters are obnoxious stereotypes (black Chris Tucker type, Survivalist Chick, Topless Bimbos, Requisite Old Couple, Asian Tour Guide) whose interactions are marred by painful, trying-to-be-hip dialogues and mostly obvious stabs at humor (not quite as bad as \"Cabin Fever,\" but still); the script has too much padding (the \"rustling bush\" scene, for example), and \"Hatchet\" winds up as typical as any postmodern slasher of the last decade, with its only distinguishing trait an expertly-calculated hype machine. I'll give it some faint praise for the gore--if you can wade through the padding in between kills, the red vino is definitely a thing of wonder, and the only real reason to watch this.\n",
      "Prediction: pos (confidence=54.05%)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "pred_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "print(classification_report(labels, pred_ids, target_names=label_names))\n",
    "\n",
    "sample_texts = raw_datasets[\"test\"][\"text\"][::max(1, len(raw_datasets[\"test\"]) // 5)]\n",
    "model.eval()\n",
    "for text in sample_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = label_names[probs.argmax(dim=-1).item()]\n",
    "    confidence = probs.max().item()\n",
    "    print(f\"\\nText: {text}\\nPrediction: {pred_label} (confidence={confidence:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a858",
   "metadata": {},
   "source": [
    "## Step 5 ? Optimize LoRA for your task\n",
    "- **Tune adapter rank (`r`) and scaling (`alpha`):** higher ranks boost capacity but increase parameters.\n",
    "- **Target additional modules:** for encoder-only models you can adapt feed-forward layers by adding their linear module names to `target_modules`.\n",
    "- **Adjust dropout:** `lora_dropout` helps regularize small datasets; try values between 0.0?0.3.\n",
    "- **Freeze embeddings or layer norms:** combine LoRA with parameter freezing by toggling `.requires_grad` on selected modules.\n",
    "- **Merge adapters for inference:** once satisfied, call `model.merge_and_unload()` to consolidate LoRA weights into the base model before exporting.\n",
    "- **Monitor resource usage:** the `print_trainable_parameters()` output is a quick guardrail to ensure fine-tuning stays lightweight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ca038",
   "metadata": {},
   "source": [
    "### Adapter merging example (optional)\n",
    "Run this after training if you need a single merged checkpoint for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6c0a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:35:46.046906Z",
     "iopub.status.busy": "2025-10-15T07:35:46.045418Z",
     "iopub.status.idle": "2025-10-15T07:35:46.540711Z",
     "shell.execute_reply": "2025-10-15T07:35:46.540711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to: artifacts/distilbert-imdb-lora\\merged\n"
     ]
    }
   ],
   "source": [
    "merged_output_dir = os.path.join(output_dir, \"merged\")\n",
    "os.makedirs(merged_output_dir, exist_ok=True)\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"Merged model saved to: {merged_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf6bfb",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Track experiments with logging tools (Weights & Biases, MLflow).\n",
    "- Evaluate robustness on adversarial or out-of-domain samples.\n",
    "- Convert the merged adapter to ONNX or TorchScript for production deployment.\n",
    "- Iterate on LoRA hyperparameters via grid or Bayesian search to balance accuracy and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
